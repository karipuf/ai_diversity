{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3afbedf6-390e-4485-a48b-fa990aba5f26",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6f1282d0-0989-45a2-bfa8-688e9137b59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import pandas as pd,numpy as np,json,re\n",
    "from json_repair import repair_json\n",
    "\n",
    "local_client=OpenAI(base_url='http://localhost:11434/v1/',api_key='12345')\n",
    "\n",
    "def topic_extraction_msgs(passage):\n",
    "  \n",
    "  return [{'role':'system',\n",
    "          'content':\n",
    "          \"\"\"\n",
    "You are a helpful assistant. Here are the instructions:\n",
    "1. The user will provide a passage of text after these instructions.\n",
    "2. Read the text and determine the main research domain or domains\n",
    "Please limit the response to  domains selected from the following list \n",
    "[\"Deep Learning\", \"Neural Networks\", \"Probabilistic methods\", \"Agents\", \"Reinforcement Learning\",\n",
    "\"Decision Trees\", \"Clustering\", \"Visualization\", \"Kernel methods\"]\n",
    "3. Provide the response in the form of a json object { \"main_topic\": topic_1, \"secondary_topic\": topic_2  }.\n",
    "4. Again, remember to use ONLY topics provided in the list above. If NONE of them fit, you can fill in \"N/A\"\n",
    "5. If there is only one matching topic, or if there is really only one main topic, set the \"secondary_topic\" to \"N/A\",   \n",
    "6. ONLY provide the json response, with no additional comments or text. \n",
    "\n",
    "          \"\"\"\n",
    "          },\n",
    "         {'role':'user',\n",
    "          'content':f\"\"\"\n",
    "Here is the passage:\n",
    "--------------------\n",
    "\n",
    "{passage}\n",
    "          \"\"\"\n",
    "          }]\n",
    "\n",
    "def extract_topics(passage,\n",
    "                  the_model='mistral-nemo:12b',\n",
    "                  client=local_client,\n",
    "                  retries=3):\n",
    "\n",
    "  for count in range(retries):\n",
    "\n",
    "    try:\n",
    "      resp=client.chat.completions.create(\n",
    "        model=the_model,\n",
    "        messages=topic_extraction_msgs(pf2010.abstract[1])\n",
    "      )\n",
    "      \n",
    "      js=json.loads(repair_json(resp.choices[0].message.content))\n",
    "      return { \n",
    "        'main_topic' : js.get(\"main_topic\",\"N/A\"),\n",
    "        'secondary_topic' : js.get(\"secondary_topic\",\"N/A\"),\n",
    "        'model' : the_model,\n",
    "        'num_retries' : count\n",
    "      }\n",
    "    except:\n",
    "      pass\n",
    "\n",
    "    # Give up\n",
    "    return { 'main_topic':'N/A' , 'secondary_topic':'N/A' , 'model':the_model, 'num_retries':-1}\n",
    "          \n",
    "\n",
    "# Loading in the data\n",
    "\n",
    "# Query openalex from the website, set source is Proceedings of the AAAI Conference on Artificial Intelligence, then year is <year>\n",
    "pf2023=pd.read_csv(\"./aaai_data/works-2024-09-06T22-53-09_2023.csv\")\n",
    "pf2020=pd.read_csv(\"./aaai_data/works-2024-09-06T21-59-15_aaai_2020.csv\")\n",
    "pf2010=pd.read_csv(\"./aaai_data/works-2024-09-06T22-01-39_aaai_2010.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "787be52c-6444-4481-97ce-63ab7d0c5bf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xw/0_t33pnn2ll9p355v7_zd_xw0000gp/T/ipykernel_20003/2323771307.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  testf['topics']=testf['abstract'].map(lambda x:extract_topics(x,the_model='mistral-nemo:12b'))    #'gemma2:9b-instruct-q5_1'))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abstract</th>\n",
       "      <th>topics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>We consider here the problem of building a never-ending language learner; that is, an intelligent computer agent that runs forever and that each day must (1) extract, or read, information from the web to populate a growing structured knowledge base, and (2) learn to perform this task better than on the previous day. In particular, we propose an approach and a set of design principles for such an agent, describe a partial implementation of such a system that has already learned to extract a knowledge base containing over 242,000 beliefs with an estimated precision of 74% after running for 67 days, and discuss lessons learned from this preliminary attempt to build a never-ending learning agent.</td>\n",
       "      <td>{'main_topic': 'Reinforcement Learning', 'secondary_topic': 'Neural Networks', 'model': 'mistral-nemo:12b', 'num_retries': 0}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Policy search is a successful approach to reinforcement learning. However, policy improvements often result in the loss of information. Hence, it has been marred by premature convergence and implausible solutions. As first suggested in the context of covariant policy gradients, many of these problems may be addressed by constraining the information loss. In this paper, we continue this path of reasoning and suggest the Relative Entropy Policy Search (REPS) method. The resulting method differs significantly from previous policy gradient approaches and yields an exact update step. It can be shown to work well on typical reinforcement learning benchmark problems.</td>\n",
       "      <td>{'main_topic': 'Reinforcement Learning', 'secondary_topic': 'Probabilistic methods', 'model': 'mistral-nemo:12b', 'num_retries': 0}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        abstract  \\\n",
       "0  We consider here the problem of building a never-ending language learner; that is, an intelligent computer agent that runs forever and that each day must (1) extract, or read, information from the web to populate a growing structured knowledge base, and (2) learn to perform this task better than on the previous day. In particular, we propose an approach and a set of design principles for such an agent, describe a partial implementation of such a system that has already learned to extract a knowledge base containing over 242,000 beliefs with an estimated precision of 74% after running for 67 days, and discuss lessons learned from this preliminary attempt to build a never-ending learning agent.   \n",
       "1                                   Policy search is a successful approach to reinforcement learning. However, policy improvements often result in the loss of information. Hence, it has been marred by premature convergence and implausible solutions. As first suggested in the context of covariant policy gradients, many of these problems may be addressed by constraining the information loss. In this paper, we continue this path of reasoning and suggest the Relative Entropy Policy Search (REPS) method. The resulting method differs significantly from previous policy gradient approaches and yields an exact update step. It can be shown to work well on typical reinforcement learning benchmark problems.   \n",
       "\n",
       "                                                                                                                                topics  \n",
       "0        {'main_topic': 'Reinforcement Learning', 'secondary_topic': 'Neural Networks', 'model': 'mistral-nemo:12b', 'num_retries': 0}  \n",
       "1  {'main_topic': 'Reinforcement Learning', 'secondary_topic': 'Probabilistic methods', 'model': 'mistral-nemo:12b', 'num_retries': 0}  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sanity check\n",
    "testf=pf2010.head(2)\n",
    "testf['topics']=testf['abstract'].map(lambda x:extract_topics(x,the_model='mistral-nemo:12b'))    #'gemma2:9b-instruct-q5_1'))\n",
    "\n",
    "pd.set_option('display.max_colwidth',None)\n",
    "testf[['abstract','topics']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fecdb0ba-5c80-4836-8277-1a551b3121da",
   "metadata": {},
   "source": [
    "#### Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f33f67-60a4-42c4-9210-19a85ac49ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics2023=pf2023.abstract[:60].map(extract_topics)\n",
    "topics2020=pf2020.abstract[:60].map(extract_topics)\n",
    "topics2010=pf2010.abstract[:60].map(extract_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3b3775-8d8a-47c4-8dba-d84f29e4a38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"2010:\\n-----\")\n",
    "print(pd.DataFrame(topics2010.tolist()).main_topic.value_counts())\n",
    "\n",
    "print(\"\\n2020:\\n-----\")\n",
    "print(pd.DataFrame(topics2020.tolist()).main_topic.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047e8c47-e33f-4d71-b11b-ce428bade4fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
